import pandas as pd
import string
import nltk
import re
pd.set_option('display.max_colwidth', 1300)
df=pd.read_csv("thisdatawilledit.csv",sep=",",usecols=[1,3,7,8,9,17],engine="python")
data=df.dropna()
data.columns=['Product','Issue','Company','State','ZIP code','Complaint ID']
def remove_punctuation(txt):
    txt_nopunct="".join([c for c in txt if c not in string.punctuation])
    return txt_nopunct
data['Product']=data['Product'].apply(lambda x: remove_punctuation(x))
data['Issue']=data['Issue'].apply(lambda x: remove_punctuation(x))
data['Company']=data['Company'].apply(lambda x: remove_punctuation(x))
data['State']=data['State'].apply(lambda x: remove_punctuation(x))

def tokenize(txt):
    tokens=re.split('\W+',txt)
    return tokens
data['Product']=data['Product'].apply(lambda x: tokenize(x.lower()) )
data['Issue']=data['Issue'].apply(lambda x: tokenize(x.lower()) )
data['Company']=data['Company'].apply(lambda x: tokenize(x.lower()))
data['State']=data['State'].apply(lambda x: tokenize(x.lower()))
stopwords=nltk.corpus.stopwords.words('english')
def remove_stopwords(txt_tokenized):
    txt_clean=" ".join([word for word in txt_tokenized if word not in stopwords])
    return txt_clean
data['Product']=data['Product'].apply(lambda x:remove_stopwords(x))
data['Issue']=data['Issue'].apply(lambda x: remove_stopwords(x))
data['Company']=data['Company'].apply(lambda x:remove_stopwords(x))
data['State']=data['State'].apply(lambda x: remove_stopwords(x))
data.to_csv("edited_data.csv")